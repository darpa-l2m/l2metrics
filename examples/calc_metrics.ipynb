{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2019 The Johns Hopkins University Applied Physics Laboratory LLC (JHU/APL).\n",
    "# All Rights Reserved. This material may be only be used, modified, or reproduced\n",
    "# by or for the U.S. Government pursuant to the license rights granted under the\n",
    "# clauses at DFARS 252.227-7013/7014 or FAR 52.227-14. For any other permission,\n",
    "# please contact the Office of Technology Transfer at JHU/APL.\n",
    "\n",
    "# NO WARRANTY, NO LIABILITY. THIS MATERIAL IS PROVIDED “AS IS.” JHU/APL MAKES NO\n",
    "# REPRESENTATION OR WARRANTY WITH RESPECT TO THE PERFORMANCE OF THE MATERIALS,\n",
    "# INCLUDING THEIR SAFETY, EFFECTIVENESS, OR COMMERCIAL VIABILITY, AND DISCLAIMS\n",
    "# ALL WARRANTIES IN THE MATERIAL, WHETHER EXPRESS OR IMPLIED, INCLUDING (BUT NOT\n",
    "# LIMITED TO) ANY AND ALL IMPLIED WARRANTIES OF PERFORMANCE, MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT OF INTELLECTUAL PROPERTY\n",
    "# OR OTHER THIRD PARTY RIGHTS. ANY USER OF THE MATERIAL ASSUMES THE ENTIRE RISK\n",
    "# AND LIABILITY FOR USING THE MATERIAL. IN NO EVENT SHALL JHU/APL BE LIABLE TO ANY\n",
    "# USER OF THE MATERIAL FOR ANY ACTUAL, INDIRECT, CONSEQUENTIAL, SPECIAL OR OTHER\n",
    "# DAMAGES ARISING FROM THE USE OF, OR INABILITY TO USE, THE MATERIAL, INCLUDING,\n",
    "# BUT NOT LIMITED TO, ANY DAMAGES FOR LOST PROFITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "from l2metrics.report import MetricsReport\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure metrics report\n",
    "log_dir = ''\n",
    "perf_measure = 'performance'\n",
    "transfer_method = 'both'\n",
    "do_smoothing = True\n",
    "do_normalize = True\n",
    "remove_outliers = True\n",
    "do_plot = True\n",
    "do_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics report\n",
    "report = MetricsReport(log_dir=log_dir, perf_measure=perf_measure,\n",
    "                       transfer_method=transfer_method, do_smoothing=do_smoothing,\n",
    "                       do_normalize=do_normalize, remove_outliers=remove_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to log data\n",
    "# report.add_noise(mean=0.0, std=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics in order of their addition to the metrics list.\n",
    "report.calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print log summary by LX and EX counts\n",
    "log_summary_df = report.log_summary()\n",
    "display(log_summary_df)\n",
    "\n",
    "print(f'\\nTotal number of experiences: {report._log_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lifetime metrics:')\n",
    "display(report.lifetime_metrics_df)\n",
    "\n",
    "print('\\nTask metrics:')\n",
    "display(report.task_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance curves for scenario\n",
    "if do_plot:\n",
    "    report.plot(save=do_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tasks and their corresponding STE\n",
    "if do_plot:\n",
    "    report.plot_ste_data(save=do_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print block summary\n",
    "print(tabulate(report.block_info, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics per regime\n",
    "print(tabulate(report.regime_metrics_df.fillna('N/A'),\n",
    "               headers='keys', tablefmt='psql', floatfmt=\".2f\"))"
   ]
  }
 ]
}