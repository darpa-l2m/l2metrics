{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2019 The Johns Hopkins University Applied Physics Laboratory LLC (JHU/APL).\n",
    "# All Rights Reserved. This material may be only be used, modified, or reproduced\n",
    "# by or for the U.S. Government pursuant to the license rights granted under the\n",
    "# clauses at DFARS 252.227-7013/7014 or FAR 52.227-14. For any other permission,\n",
    "# please contact the Office of Technology Transfer at JHU/APL.\n",
    "\n",
    "# NO WARRANTY, NO LIABILITY. THIS MATERIAL IS PROVIDED “AS IS.” JHU/APL MAKES NO\n",
    "# REPRESENTATION OR WARRANTY WITH RESPECT TO THE PERFORMANCE OF THE MATERIALS,\n",
    "# INCLUDING THEIR SAFETY, EFFECTIVENESS, OR COMMERCIAL VIABILITY, AND DISCLAIMS\n",
    "# ALL WARRANTIES IN THE MATERIAL, WHETHER EXPRESS OR IMPLIED, INCLUDING (BUT NOT\n",
    "# LIMITED TO) ANY AND ALL IMPLIED WARRANTIES OF PERFORMANCE, MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT OF INTELLECTUAL PROPERTY\n",
    "# OR OTHER THIRD PARTY RIGHTS. ANY USER OF THE MATERIAL ASSUMES THE ENTIRE RISK\n",
    "# AND LIABILITY FOR USING THE MATERIAL. IN NO EVENT SHALL JHU/APL BE LIABLE TO ANY\n",
    "# USER OF THE MATERIAL FOR ANY ACTUAL, INDIRECT, CONSEQUENTIAL, SPECIAL OR OTHER\n",
    "# DAMAGES ARISING FROM THE USE OF, OR INABILITY TO USE, THE MATERIAL, INCLUDING,\n",
    "# BUT NOT LIMITED TO, ANY DAMAGES FOR LOST PROFITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from l2metrics.report import MetricsReport\n",
    "from tabulate import tabulate\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure metrics report\n",
    "kwargs = {}\n",
    "kwargs['log_dir'] = 'multi_task'\n",
    "kwargs['ste_averaging_method']='time'\n",
    "kwargs['perf_measure'] = 'performance'\n",
    "kwargs['aggregation_method'] = 'mean'\n",
    "kwargs['maintenance_method'] = 'both'\n",
    "kwargs['transfer_method'] = 'both'\n",
    "kwargs['normalization_method'] = 'task'\n",
    "kwargs['smoothing_method'] = 'flat'\n",
    "kwargs['window_length'] = None\n",
    "kwargs['clamp_outliers'] = True\n",
    "kwargs['data_range_file'] = None # 'data_range.json'\n",
    "kwargs['show_raw_data'] = True\n",
    "kwargs['show_eval_lines'] = True\n",
    "kwargs['do_plot'] = True\n",
    "kwargs['do_save'] = False\n",
    "kwargs['do_save_settings'] = False\n",
    "\n",
    "output_dir = Path('results')\n",
    "output = 'll_metrics'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if kwargs['do_save'] or kwargs['do_save_settings']:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data range data for normalization and standardize names to lowercase\n",
    "if kwargs['data_range_file']:\n",
    "    with open(kwargs['data_range_file']) as config_json:\n",
    "        data_range = json.load(config_json)\n",
    "        data_range = {key.lower(): val for key, val in data_range.items()}\n",
    "else:\n",
    "    data_range = None\n",
    "\n",
    "kwargs['data_range'] = data_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics report\n",
    "report = MetricsReport(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to log data\n",
    "# report.add_noise(mean=0.0, std=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics in order of their addition to the metrics list.\n",
    "report.calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print log summary by LX and EX counts\n",
    "log_summary_df = report.log_summary()\n",
    "display(log_summary_df)\n",
    "\n",
    "print(f'\\nTotal number of experiences: {report._log_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lifetime metrics:')\n",
    "display(report.lifetime_metrics_df)\n",
    "\n",
    "print('\\nTask metrics:')\n",
    "display(report.task_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance curves for scenario\n",
    "if kwargs['do_plot']:\n",
    "    report.plot(save=kwargs['do_save'], show_raw_data=kwargs['show_raw_data'], \\\n",
    "        show_eval_lines=kwargs['show_eval_lines'], output_dir=str(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tasks and their corresponding STE\n",
    "if kwargs['do_plot']:\n",
    "    report.plot_ste_data(save=kwargs['do_save'], output_dir=str(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to feather file\n",
    "report.save_data(output_dir=str(output_dir), filename=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print block summary\n",
    "print(tabulate(report.block_info, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics per regime\n",
    "print(tabulate(report.regime_metrics_df.fillna('N/A'),\n",
    "               headers='keys', tablefmt='psql', floatfmt=\".2f\"))"
   ]
  }
 ]
}
