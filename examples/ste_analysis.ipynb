{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright © 2021-2022 The Johns Hopkins University Applied Physics Laboratory LLC\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy \n",
    "of this software and associated documentation files (the “Software”), to \n",
    "deal in the Software without restriction, including without limitation the \n",
    "rights to use, copy, modify, merge, publish, distribute, sublicense, and/or \n",
    "sell copies of the Software, and to permit persons to whom the Software is \n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in \n",
    "all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n",
    "WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR \n",
    "IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STE Analysis Notebook\n",
    "\n",
    "This notebook is intended to perform analysis on STE runs, focusing on saturation value, experience\n",
    "\n",
    "to saturation, and distribution of both values across multiple runs. The notebook also generates\n",
    "\n",
    "lines plots for each task variant with mean and 95% confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import l2logger.util as l2l\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from l2metrics._localutil import get_block_saturation_perf, smooth\n",
    "from l2metrics.normalizer import Normalizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib ipympl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to log data\n",
    "# UPDATE THIS PATH\n",
    "log_dir = Path(\".\")\n",
    "\n",
    "# Define path to analysis directory for outputs\n",
    "analysis_dir = log_dir / \"analysis\"\n",
    "\n",
    "# Create analysis directory if it doesn't exist\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Analysis settings\n",
    "do_save = True\n",
    "units = \"steps\"  # {\"exp_num\", \"steps\"}\n",
    "\n",
    "# L2Metrics settings\n",
    "perf_measure = \"reward\"\n",
    "smoothing_method = \"flat\"\n",
    "window_len = None\n",
    "normalization_method = \"task\"\n",
    "\n",
    "# UPDATE THIS PATH\n",
    "data_range_file = Path(\"m21_data_range.json\")\n",
    "\n",
    "# Load data range data for normalization and standardize names to lowercase\n",
    "with open(data_range_file) as drf:\n",
    "    data_range = json.load(drf)\n",
    "    data_range = {key.lower(): val for key, val in data_range.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M21 task name map\n",
    "task_name_remap = {\n",
    "    \"MiniGridSimpleCrossingS9N1\": \"simplecrossing_s9n1\",\n",
    "    \"MiniGridSimpleCrossingS9N2\": \"simplecrossing_s9n2\",\n",
    "    \"MiniGridSimpleCrossingS9N3\": \"simplecrossing_s9n3\",\n",
    "    \"MiniGridDistShiftR2\": \"distshift_r2\",\n",
    "    \"MiniGridDistShiftR5\": \"distshift_r5\",\n",
    "    \"MiniGridDistShiftR3\": \"distshift_r3\",\n",
    "    \"MiniGridDynObstaclesS6N1\": \"dynobstacles_s6n1\",\n",
    "    \"MiniGridDynObstaclesS8N2\": \"dynobstacles_s8n2\",\n",
    "    \"MiniGridDynObstaclesS10N3\": \"dynobstacles_s10n3\",\n",
    "    \"MiniGridCustomFetchS5T1N2\": \"customfetch_s5t1n2\",\n",
    "    \"MiniGridCustomFetchS8T1N2\": \"customfetch_s8t1n2\",\n",
    "    \"MiniGridCustomFetchS10T2N4\": \"customfetch_s10t2n4\",\n",
    "    \"MiniGridCustomUnlockS5\": \"customunlock_s5\",\n",
    "    \"MiniGridCustomUnlockS7\": \"customunlock_s7\",\n",
    "    \"MiniGridCustomUnlockS9\": \"customunlock_s9\",\n",
    "    \"MiniGridDoorKeyS5\": \"doorkey_s5\",\n",
    "    \"MiniGridDoorKeyS6\": \"doorkey_s6\",\n",
    "    \"MiniGridDoorKeyS7\": \"doorkey_s7\",\n",
    "}\n",
    "\n",
    "# Define list of top-level tasks\n",
    "task_clusters = [\n",
    "    \"simplecrossing\",\n",
    "    \"distshift\",\n",
    "    \"dynobstacles\",\n",
    "    \"customfetch\",\n",
    "    \"customunlock\",\n",
    "    \"doorkey\",\n",
    "]\n",
    "\n",
    "# Define list of task variant names\n",
    "task_names = list(task_name_remap.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame\n",
    "tb_df = pd.DataFrame()\n",
    "train_tb_interp_df = pd.DataFrame()\n",
    "\n",
    "resampled_step = np.linspace(100, 1_000_000, 10_000)\n",
    "\n",
    "# Get directories in path to log data\n",
    "dirs = [p for p in log_dir.glob(\"*\") if p.is_dir()]\n",
    "\n",
    "# Iterate over log directories\n",
    "for dir in tqdm(dirs):\n",
    "    # Read logger info\n",
    "    try:\n",
    "        logger_info = l2l.read_logger_info(dir)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"{dir.name}: Logger info file not found!\")\n",
    "        continue\n",
    "\n",
    "    # Check if performance measure exists in logger info\n",
    "    if perf_measure not in logger_info[\"metrics_columns\"]:\n",
    "        print(\n",
    "            f\"{dir.name}: Performance measure ({perf_measure}) not found in valid metrics columns: {logger_info['metrics_columns']}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Parse log data from directory\n",
    "    log_data = l2l.read_log_data(dir)\n",
    "\n",
    "    # Check if performance measure is logged\n",
    "    if perf_measure not in log_data.columns:\n",
    "        print(\n",
    "            f\"{dir.name}: Performance measure ({perf_measure}) not found in the log data\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Validate data format\n",
    "    try:\n",
    "        l2l.validate_log(log_data, logger_info[\"metrics_columns\"])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"{dir.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Filter data by completed experiences\n",
    "    log_data = log_data[log_data[\"exp_status\"] == \"complete\"]\n",
    "\n",
    "    # Drop all rows with NaN values\n",
    "    log_data = log_data[log_data[perf_measure].notna()]\n",
    "\n",
    "    # Check for log data after filtering\n",
    "    if log_data.empty:\n",
    "        print(f\"{dir.name}: Logs do not contain any valid data for: {perf_measure}\")\n",
    "        continue\n",
    "\n",
    "    # Fill in regime number and sort\n",
    "    log_data = l2l.fill_regime_num(log_data)\n",
    "    log_data = log_data.sort_values(by=[\"regime_num\", \"exp_num\"]).set_index(\n",
    "        \"regime_num\", drop=False\n",
    "    )\n",
    "\n",
    "    # Get block summary\n",
    "    block_info = l2l.parse_blocks(log_data)\n",
    "\n",
    "    # Drop unused columns\n",
    "    log_data.drop(\n",
    "        columns=[\"worker_id\", \"block_subtype\", \"task_params\", \"exp_status\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Smooth data\n",
    "    for regime_num in block_info[\"regime_num\"].unique():\n",
    "        if block_info.iloc[regime_num].block_type == \"train\":\n",
    "            log_data.loc[log_data[\"regime_num\"] == regime_num, perf_measure] = smooth(\n",
    "                log_data.loc[\n",
    "                    log_data[\"regime_num\"] == regime_num, perf_measure\n",
    "                ].to_numpy(),\n",
    "                window_len=window_len,\n",
    "                window=smoothing_method,\n",
    "            )\n",
    "\n",
    "    # Normalize data\n",
    "    if normalization_method != \"none\":\n",
    "        # Instantiate normalizer\n",
    "        normalizer = Normalizer(\n",
    "            perf_measure=perf_measure,\n",
    "            data=log_data[[\"task_name\", perf_measure]].set_index(\"task_name\"),\n",
    "            data_range=data_range,\n",
    "            method=normalization_method,\n",
    "        )\n",
    "\n",
    "        log_data = normalizer.normalize(log_data)\n",
    "\n",
    "    # Add run ID to log data\n",
    "    log_data.insert(loc=0, column=\"run_id\", value=dir.name)\n",
    "\n",
    "    # Compute within-block episode and step numbers\n",
    "    log_data.insert(loc=3, column=\"step_num\", value=0)\n",
    "\n",
    "    for regime_num in block_info[\"regime_num\"].unique():\n",
    "        log_data.loc[log_data[\"regime_num\"] == regime_num, \"step_num\"] = log_data.loc[\n",
    "            log_data[\"regime_num\"] == regime_num, \"episode_step_count\"\n",
    "        ].cumsum()\n",
    "\n",
    "    tb_df = pd.concat([tb_df, log_data], ignore_index=True)\n",
    "\n",
    "    train_regime = log_data.loc[log_data[\"block_type\"] == \"train\"]\n",
    "    train_tb_interp_df = pd.concat(\n",
    "        [\n",
    "            train_tb_interp_df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"run_id\": dir.name,\n",
    "                    \"task_name\": train_regime[\"task_name\"].unique()[0],\n",
    "                    \"steps\": resampled_step,\n",
    "                    \"reward\": np.interp(\n",
    "                        resampled_step,\n",
    "                        train_regime[\"step_num\"].to_numpy(),\n",
    "                        train_regime[\"reward\"].to_numpy(),\n",
    "                    ),\n",
    "                }\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "train_tb_df = tb_df[tb_df[\"block_type\"] == \"train\"]\n",
    "eval_tb_df = tb_df[tb_df[\"block_type\"] == \"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tasks = [\n",
    "    \"distshift_r2\",\n",
    "    \"distshift_r5\",\n",
    "    \"distshift_r3\",\n",
    "    \"dynobstacles_s6n1\",\n",
    "    \"dynobstacles_s8n2\",\n",
    "    \"dynobstacles_s10n3\",\n",
    "]\n",
    "\n",
    "start_perf = {}\n",
    "\n",
    "for neg_task in neg_tasks:\n",
    "    start_perf[neg_task] = (\n",
    "        train_tb_interp_df[train_tb_interp_df[\"task_name\"] == neg_task]\n",
    "        .groupby(\"run_id\")\n",
    "        .head(10)[\"reward\"]\n",
    "        .mean()\n",
    "    ) - 1\n",
    "\n",
    "print(start_perf)\n",
    "\n",
    "clamps = {}\n",
    "\n",
    "for task, perf in start_perf.items():\n",
    "    clamps[task] = (50 - perf) / 50 * -1\n",
    "\n",
    "print(clamps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize task chracteristic DataFrame\n",
    "task_char_df = pd.DataFrame()\n",
    "\n",
    "for run_id in tqdm(train_tb_df[\"run_id\"].unique()):\n",
    "    task_char_dict = {}\n",
    "\n",
    "    run_df = train_tb_df[(train_tb_df[\"run_id\"] == run_id)]\n",
    "\n",
    "    if run_df.size > 0:\n",
    "        # Get task variant reward data for current run\n",
    "        data = run_df[\"reward\"].to_numpy()\n",
    "\n",
    "        # Get task variant step data for current run\n",
    "        step_data = run_df[\"step_num\"].to_numpy()\n",
    "\n",
    "        # Get task variant saturation metrics for current run\n",
    "        sat_val, exp_to_sat, _ = get_block_saturation_perf(data, window_len=5)\n",
    "\n",
    "        task_char_dict[\"task_name\"] = run_df[\"task_name\"].unique()[0]\n",
    "        task_char_dict[\"sat_val\"] = sat_val\n",
    "        task_char_dict[\"exp_to_sat\"] = step_data[exp_to_sat]\n",
    "\n",
    "        # Append metrics to DataFrame\n",
    "        task_char_df = pd.concat(\n",
    "            [task_char_df, pd.DataFrame(task_char_dict, index=[0])], ignore_index=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"{run_id}: No training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for STE characteristics\n",
    "summary_df = task_char_df.groupby(\"task_name\").agg(\n",
    "    [\"mean\", \"std\", \"median\", \"min\", \"max\", \"count\"]\n",
    ")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify tasks that did not learn above 50% of max performance\n",
    "summary_df[summary_df[(\"sat_val\", \"mean\")] < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save task characteristic summary\n",
    "if do_save:\n",
    "    summary_df.columns = summary_df.columns.map(\"_\".join).str.strip(\"_\")\n",
    "    summary_df.to_csv(analysis_dir / \"characteristics.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transfer DataFrame\n",
    "transfer_df = pd.DataFrame()\n",
    "\n",
    "# Convert evaluation dictionaries to DataFrame\n",
    "for run_id in tqdm(eval_tb_df[\"run_id\"].unique()):\n",
    "    transfer_row = {}\n",
    "    transfer_row[\"base_task\"] = task_name_remap[run_id.split(\"-\", 1)[0]]\n",
    "\n",
    "    run_df = eval_tb_df[(eval_tb_df[\"run_id\"] == run_id)]\n",
    "\n",
    "    # Check for two evaluation blocks\n",
    "    block_nums = run_df[\"block_num\"].unique()\n",
    "    if len(block_nums) == 2:\n",
    "        transfer_tasks = run_df[\"task_name\"].unique()\n",
    "\n",
    "        for transfer_task in transfer_tasks:\n",
    "            transfer_row[\"transfer_task\"] = transfer_task\n",
    "\n",
    "            p0 = run_df[\n",
    "                (run_df[\"block_num\"] == block_nums[0])\n",
    "                & (run_df[\"task_name\"] == transfer_task)\n",
    "            ][\"reward\"].mean()\n",
    "            p1 = run_df[\n",
    "                (run_df[\"block_num\"] == block_nums[1])\n",
    "                & (run_df[\"task_name\"] == transfer_task)\n",
    "            ][\"reward\"].mean()\n",
    "\n",
    "            transfer_row[\"p0\"] = p0\n",
    "            transfer_row[\"p1\"] = p1\n",
    "\n",
    "            transfer_row[\"forward_transfer_ratio\"] = p1 / p0\n",
    "            transfer_row[\"forward_transfer_contrast\"] = (p1 - p0) / (p1 + p0)\n",
    "            transfer_df = pd.concat(\n",
    "                [transfer_df, pd.DataFrame(transfer_row, index=[0])], ignore_index=True\n",
    "            )\n",
    "    else:\n",
    "        print(f\"{run_id}: Invalid number of evaluation blocks to compute transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average transfer values\n",
    "avg_transfer_df = (\n",
    "    transfer_df.groupby(by=[\"base_task\", \"transfer_task\"]).agg([\"mean\"]).reset_index()\n",
    ")\n",
    "avg_transfer_df.columns = avg_transfer_df.columns.droplevel(1)\n",
    "avg_transfer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full transfer matrix\n",
    "full_transfer_df = pd.DataFrame(columns=task_names)\n",
    "\n",
    "for task_name in task_names:\n",
    "    transfer_row = {}\n",
    "\n",
    "    temp_df = avg_transfer_df[avg_transfer_df[\"base_task\"] == task_name]\n",
    "\n",
    "    for _, row in temp_df.iterrows():\n",
    "        transfer_row[row[\"transfer_task\"]] = row[\"forward_transfer_contrast\"]\n",
    "\n",
    "    full_transfer_df = pd.concat(\n",
    "        [full_transfer_df, pd.DataFrame(transfer_row, index=[task_name])]\n",
    "    )\n",
    "\n",
    "full_transfer_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transfer matrix\n",
    "if do_save:\n",
    "    full_transfer_df.to_csv(analysis_dir / \"full_transfer.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get across-task variant-pair transfers\n",
    "across_transfer_df = pd.DataFrame(columns=task_clusters)\n",
    "\n",
    "for base_task_cluster in task_clusters:\n",
    "    across_transfer_row = {}\n",
    "    for transfer_task_cluster in task_clusters:\n",
    "        across_transfer_row[transfer_task_cluster] = avg_transfer_df[\n",
    "            avg_transfer_df[\"base_task\"].str.contains(base_task_cluster)\n",
    "            & avg_transfer_df[\"transfer_task\"].str.contains(transfer_task_cluster)\n",
    "        ][\"forward_transfer_contrast\"].mean()\n",
    "    across_transfer_df = pd.concat(\n",
    "        [\n",
    "            across_transfer_df,\n",
    "            pd.DataFrame(across_transfer_row, index=[base_task_cluster]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "across_transfer_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "ste_data = pd.DataFrame()\n",
    "\n",
    "for task_cluster in tqdm(task_clusters):\n",
    "    if units == \"steps\":\n",
    "        ste_data = train_tb_interp_df[\n",
    "            train_tb_interp_df[\"task_name\"].str.contains(task_cluster)\n",
    "        ]\n",
    "    elif units == \"exp_num\":\n",
    "        ste_data = train_tb_df[train_tb_df[\"task_name\"].str.contains(task_cluster)]\n",
    "\n",
    "    if not ste_data.empty:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6), constrained_layout=True)\n",
    "        sns.lineplot(data=ste_data, hue=\"task_name\", x=units, y=\"reward\", ax=ax)\n",
    "        ax.title.set_text(task_cluster)\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "        if do_save:\n",
    "            plt.savefig(analysis_dir / (task_cluster + \".png\"))\n",
    "    else:\n",
    "        print(f\"No data for task cluster: {task_cluster}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a05ad442db9bc9704899665875cc9bbf95b98f5745eca7921a57399c2fc1ca7b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv383': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
