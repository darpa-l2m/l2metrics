{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-Lifetime Metrics Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2019 The Johns Hopkins University Applied Physics Laboratory LLC (JHU/APL).\n",
    "# All Rights Reserved. This material may be only be used, modified, or reproduced\n",
    "# by or for the U.S. Government pursuant to the license rights granted under the\n",
    "# clauses at DFARS 252.227-7013/7014 or FAR 52.227-14. For any other permission,\n",
    "# please contact the Office of Technology Transfer at JHU/APL.\n",
    "\n",
    "# NO WARRANTY, NO LIABILITY. THIS MATERIAL IS PROVIDED “AS IS.” JHU/APL MAKES NO\n",
    "# REPRESENTATION OR WARRANTY WITH RESPECT TO THE PERFORMANCE OF THE MATERIALS,\n",
    "# INCLUDING THEIR SAFETY, EFFECTIVENESS, OR COMMERCIAL VIABILITY, AND DISCLAIMS\n",
    "# ALL WARRANTIES IN THE MATERIAL, WHETHER EXPRESS OR IMPLIED, INCLUDING (BUT NOT\n",
    "# LIMITED TO) ANY AND ALL IMPLIED WARRANTIES OF PERFORMANCE, MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT OF INTELLECTUAL PROPERTY\n",
    "# OR OTHER THIRD PARTY RIGHTS. ANY USER OF THE MATERIAL ASSUMES THE ENTIRE RISK\n",
    "# AND LIABILITY FOR USING THE MATERIAL. IN NO EVENT SHALL JHU/APL BE LIABLE TO ANY\n",
    "# USER OF THE MATERIAL FOR ANY ACTUAL, INDIRECT, CONSEQUENTIAL, SPECIAL OR OTHER\n",
    "# DAMAGES ARISING FROM THE USE OF, OR INABILITY TO USE, THE MATERIAL, INCLUDING,\n",
    "# BUT NOT LIMITED TO, ANY DAMAGES FOR LOST PROFITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from evaluation.evaluate import (compute_eval_metrics,\n",
    "                                 load_computational_costs,\n",
    "                                 load_performance_thresholds,\n",
    "                                 load_task_similarities, plot_summary,\n",
    "                                 save_ste_data, unzip_logs)\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify top-level directory for evaluation\n",
    "# The path should be that of an evaluation directory (e.g., example_eval/m9_eval/).\n",
    "log_dir = Path(\"example_eval/m9_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure metrics report\n",
    "ste_dir = 'agent_config-0'\n",
    "perf_measure = 'performance'\n",
    "transfer_method = 'both'\n",
    "output_dir = Path('sg_results')\n",
    "output = 'll_metrics.tsv'\n",
    "do_unzip = False\n",
    "do_smoothing = True\n",
    "do_normalize = True\n",
    "remove_outliers = True\n",
    "save_ste = True\n",
    "do_plot = True\n",
    "save_plots = True\n",
    "do_save = True\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load computational cost data\n",
    "# comp_cost_df = load_computational_costs(eval_dir)\n",
    "# comp_cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance threshold data\n",
    "# perf_thresh_df = load_performance_thresholds(eval_dir)\n",
    "# perf_thresh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task similarity data\n",
    "# task_similarity_df = load_task_similarities(eval_dir)\n",
    "# task_similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip logs\n",
    "if do_unzip:\n",
    "    unzip_logs(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for lifelong learning logs\n",
    "matplotlib.use('Agg')\n",
    "ll_metrics_df = compute_eval_metrics(eval_dir=eval_dir, ste_dir=ste_dir, output_dir=output_dir,\n",
    "                                     perf_measure=perf_measure, transfer_method=transfer_method,\n",
    "                                     do_smoothing=do_smoothing, do_normalize=do_normalize,\n",
    "                                     remove_outliers=remove_outliers, save_plots=save_plots,\n",
    "                                     do_save_ste=save_ste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mean and standard deviation of data\n",
    "ll_metrics_df.drop(columns=['min', 'max']).groupby(by=['complexity', 'difficulty']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show median and IQR of data\n",
    "ll_metrics_df.drop(columns=['min', 'max']).groupby(by=['complexity', 'difficulty']).agg(['median', scipy.stats.iqr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aggregated data\n",
    "if do_plot:\n",
    "    matplotlib.use('TkAgg')\n",
    "    plot(ll_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lifelong learning metrics DataFrame\n",
    "if do_save:\n",
    "    with open(output_dir / output, 'w', newline='\\n') as metrics_file:\n",
    "        ll_metrics_df.set_index(['sg_name', 'agent_config', 'run_id']).sort_values(['agent_config', 'run_id']).to_csv(metrics_file, sep='\\t')"
   ]
  }
 ]
}