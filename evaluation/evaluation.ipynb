{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-Lifetime Metrics Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2019 The Johns Hopkins University Applied Physics Laboratory LLC (JHU/APL).\n",
    "# All Rights Reserved. This material may be only be used, modified, or reproduced\n",
    "# by or for the U.S. Government pursuant to the license rights granted under the\n",
    "# clauses at DFARS 252.227-7013/7014 or FAR 52.227-14. For any other permission,\n",
    "# please contact the Office of Technology Transfer at JHU/APL.\n",
    "\n",
    "# NO WARRANTY, NO LIABILITY. THIS MATERIAL IS PROVIDED “AS IS.” JHU/APL MAKES NO\n",
    "# REPRESENTATION OR WARRANTY WITH RESPECT TO THE PERFORMANCE OF THE MATERIALS,\n",
    "# INCLUDING THEIR SAFETY, EFFECTIVENESS, OR COMMERCIAL VIABILITY, AND DISCLAIMS\n",
    "# ALL WARRANTIES IN THE MATERIAL, WHETHER EXPRESS OR IMPLIED, INCLUDING (BUT NOT\n",
    "# LIMITED TO) ANY AND ALL IMPLIED WARRANTIES OF PERFORMANCE, MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT OF INTELLECTUAL PROPERTY\n",
    "# OR OTHER THIRD PARTY RIGHTS. ANY USER OF THE MATERIAL ASSUMES THE ENTIRE RISK\n",
    "# AND LIABILITY FOR USING THE MATERIAL. IN NO EVENT SHALL JHU/APL BE LIABLE TO ANY\n",
    "# USER OF THE MATERIAL FOR ANY ACTUAL, INDIRECT, CONSEQUENTIAL, SPECIAL OR OTHER\n",
    "# DAMAGES ARISING FROM THE USE OF, OR INABILITY TO USE, THE MATERIAL, INCLUDING,\n",
    "# BUT NOT LIMITED TO, ANY DAMAGES FOR LOST PROFITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "from evaluation.evaluate import (compute_eval_metrics,\n",
    "                                 load_computational_costs,\n",
    "                                 load_performance_thresholds,\n",
    "                                 load_task_similarities,\n",
    "                                 unzip_logs)\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure metrics report\n",
    "kwargs = {}\n",
    "sg_name = 'example'\n",
    "configuration = 'normalized_no_outliers'\n",
    "\n",
    "# Build file and directory strings\n",
    "kwargs['eval_dir'] = Path('../../' + sg_name + '_eval/m12_eval/')\n",
    "kwargs['output_dir'] = Path('results/' + configuration +\n",
    "                  '/' + sg_name + '_' + configuration)\n",
    "output = sg_name + '_metrics_' + configuration\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "kwargs['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "perf_measure = {\n",
    "    'argonne': 'score',\n",
    "    'hrl': 'norm_reward',\n",
    "    'sri': 'reward',\n",
    "    'teledyne': 'id_accuracy_cumulative',\n",
    "    'upenn': 'performance',\n",
    "    'example': 'performance'\n",
    "}\n",
    "\n",
    "kwargs['ste_dir'] = 'agent_config'\n",
    "kwargs['ste_averaging_method'] = 'metrics'\n",
    "kwargs['perf_measure'] = perf_measure[sg_name]\n",
    "kwargs['aggregation_method'] = 'median'\n",
    "kwargs['maintenance_method'] = 'both'\n",
    "kwargs['transfer_method'] = 'both'\n",
    "kwargs['normalization_method'] = 'task'\n",
    "# kwargs['data_range_file'] = 'data_range.json'\n",
    "kwargs['show_raw_data'] = True\n",
    "kwargs['do_store_ste'] = True\n",
    "kwargs['do_plot'] = True\n",
    "kwargs['do_save_plots'] = True\n",
    "kwargs['do_save'] = True\n",
    "kwargs['save_config'] = True\n",
    "do_unzip = False\n",
    "\n",
    "# Generate other input arguments based on configuration\n",
    "kwargs['smoothing_method'] = 'flat' if config in [\n",
    "        'smoothed', 'normalized', 'normalized_no_outliers'] else 'none'\n",
    "kwargs['remove_outliers'] = config in ['normalized_no_outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip logs\n",
    "if do_unzip:\n",
    "    unzip_logs(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for lifelong learning logs\n",
    "ll_metrics_df, ll_metrics_dicts = compute_eval_metrics(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mean and standard deviation of data\n",
    "ll_metrics_df.drop(columns=['min', 'max', 'num_lx', 'num_ex']).groupby(\n",
    "    by=['scenario_type', 'complexity', 'difficulty']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show median and IQR of data\n",
    "ll_metrics_df.drop(columns=['min', 'max', 'num_lx', 'num_ex']).groupby(\n",
    "    by=['scenario_type', 'complexity', 'difficulty']).agg(['median', scipy.stats.iqr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lifelong learning metrics DataFrame\n",
    "if kwargs['do_save']:\n",
    "    with open(kwargs['output_dir'].parent / (output + '.tsv'), 'w', newline='\\n') as metrics_file:\n",
    "        ll_metrics_df.set_index(['sg_name', 'agent_config', 'run_id']).sort_values(\n",
    "            ['agent_config', 'run_id']).to_csv(metrics_file, sep='\\t')\n",
    "    with open(kwargs['output_dir'].parent / (output + '.json'), 'w', newline='\\n') as metrics_file:\n",
    "        json.dump(ll_metrics_dicts, metrics_file)"
   ]
  }
 ]
}