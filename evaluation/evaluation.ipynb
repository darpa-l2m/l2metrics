{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-Lifetime Metrics Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2019 The Johns Hopkins University Applied Physics Laboratory LLC (JHU/APL).\n",
    "# All Rights Reserved. This material may be only be used, modified, or reproduced\n",
    "# by or for the U.S. Government pursuant to the license rights granted under the\n",
    "# clauses at DFARS 252.227-7013/7014 or FAR 52.227-14. For any other permission,\n",
    "# please contact the Office of Technology Transfer at JHU/APL.\n",
    "\n",
    "# NO WARRANTY, NO LIABILITY. THIS MATERIAL IS PROVIDED “AS IS.” JHU/APL MAKES NO\n",
    "# REPRESENTATION OR WARRANTY WITH RESPECT TO THE PERFORMANCE OF THE MATERIALS,\n",
    "# INCLUDING THEIR SAFETY, EFFECTIVENESS, OR COMMERCIAL VIABILITY, AND DISCLAIMS\n",
    "# ALL WARRANTIES IN THE MATERIAL, WHETHER EXPRESS OR IMPLIED, INCLUDING (BUT NOT\n",
    "# LIMITED TO) ANY AND ALL IMPLIED WARRANTIES OF PERFORMANCE, MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT OF INTELLECTUAL PROPERTY\n",
    "# OR OTHER THIRD PARTY RIGHTS. ANY USER OF THE MATERIAL ASSUMES THE ENTIRE RISK\n",
    "# AND LIABILITY FOR USING THE MATERIAL. IN NO EVENT SHALL JHU/APL BE LIABLE TO ANY\n",
    "# USER OF THE MATERIAL FOR ANY ACTUAL, INDIRECT, CONSEQUENTIAL, SPECIAL OR OTHER\n",
    "# DAMAGES ARISING FROM THE USE OF, OR INABILITY TO USE, THE MATERIAL, INCLUDING,\n",
    "# BUT NOT LIMITED TO, ANY DAMAGES FOR LOST PROFITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "from evaluation.evaluate import (compute_eval_metrics,\n",
    "                                 load_computational_costs,\n",
    "                                 load_performance_thresholds,\n",
    "                                 load_task_similarities,\n",
    "                                 unzip_logs)\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "source": [
    "## SG-Specific Application Measures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M9\n",
    "# perf_measures = {\n",
    "#     'argonne': 'score',\n",
    "#     'hrl': 'norm_reward',\n",
    "#     'sri': 'reward',\n",
    "#     'teledyne': 'id_accuracy_incremental',\n",
    "#     'upenn': 'performance'\n",
    "# }\n",
    "\n",
    "# M12\n",
    "perf_measures = {\n",
    "    'argonne': 'score',\n",
    "    'hrl': 'reward',\n",
    "    'sri': 'reward',\n",
    "    'teledyne': 'object_id_accuracy',\n",
    "    'upenn': 'performance'\n",
    "}"
   ]
  },
  {
   "source": [
    "## Configure Metrics Report"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the three values below to configure metrics report based on processing mode\n",
    "eval_dir = 'm12_eval'\n",
    "sg_name = ''\n",
    "processing_mode = 'raw' # Valid modes: 'raw', 'smoothed', 'normalized', 'normalized_no_outliers'\n",
    "\n",
    "# L2Metrics settings\n",
    "kwargs = {}\n",
    "kwargs['eval_dir'] = Path('../../sg_' + sg_name + '_eval/' + eval_dir)\n",
    "kwargs['output_dir'] = Path('results/' + processing_mode + '/' + sg_name)\n",
    "kwargs['ste_dir'] = 'agent_config'\n",
    "kwargs['ste_averaging_method'] = 'time'\n",
    "kwargs['perf_measure'] = perf_measures[sg_name]\n",
    "kwargs['aggregation_method'] = 'mean'\n",
    "kwargs['maintenance_method'] = 'both'\n",
    "kwargs['transfer_method'] = 'both'\n",
    "kwargs['window_length'] = None\n",
    "kwargs['show_raw_data'] = True\n",
    "kwargs['show_eval_lines'] = True\n",
    "kwargs['do_store_ste'] = False\n",
    "kwargs['do_plot'] = True\n",
    "kwargs['do_save_plots'] = True\n",
    "kwargs['do_save'] = True\n",
    "kwargs['do_save_settings'] = True\n",
    "output = sg_name + '_' + processing_mode\n",
    "do_unzip = False\n",
    "\n",
    "# Generate other input arguments based on data processing mode\n",
    "kwargs['normalization_method'] = 'task' if processing_mode in [\n",
    "    'normalized', 'normalized_no_outliers'] else 'none'\n",
    "kwargs['smoothing_method'] = 'flat' if processing_mode in [\n",
    "    'smoothed', 'normalized', 'normalized_no_outliers'] else 'none'\n",
    "kwargs['clamp_outliers'] = processing_mode in ['normalized_no_outliers']\n",
    "\n",
    "# Load data range data for normalization and standardize names to lowercase\n",
    "if sg_name == 'sri':\n",
    "    with open('sri_data_range.json') as f:\n",
    "        data_range = json.load(f)\n",
    "        data_range = {key.lower(): val for key, val in data_range.items()}\n",
    "else:\n",
    "    data_range = None\n",
    "kwargs['data_range'] = data_range\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if kwargs['do_save_plots'] or kwargs['do_save'] or kwargs['do_save_settings']:\n",
    "    kwargs['output_dir'].mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "source": [
    "## Unzip Logs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_unzip:\n",
    "    unzip_logs(eval_dir)"
   ]
  },
  {
   "source": [
    "## Compute Metrics for Evaluation\n",
    "\n",
    "This line of code runs through all the logs in the specified evaluation directory, stores the STE data (if enabled),\n",
    "then computes metrics on the LL logs with the settings above. The lifetime and task-level metrics for each run are\n",
    "aggregated into a single DataFrame and dictionary, respectively. The aggregated log data from each run is also returned\n",
    "as a DataFrame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_metrics_df, ll_metrics_dicts, log_data_df = compute_eval_metrics(**kwargs)"
   ]
  },
  {
   "source": [
    "## Summary Report"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mean and standard deviation of data\n",
    "ll_metrics_df.drop(columns=['min', 'max', 'num_lx', 'num_ex']).groupby(\n",
    "    by=['scenario_type', 'complexity', 'difficulty']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show median and IQR of data\n",
    "ll_metrics_df.drop(columns=['min', 'max', 'num_lx', 'num_ex']).groupby(\n",
    "    by=['scenario_type', 'complexity', 'difficulty']).agg(['median', scipy.stats.iqr])"
   ]
  },
  {
   "source": [
    "## Save Metrics and Log Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lifelong learning metrics DataFrame\n",
    "if kwargs['do_save']:\n",
    "    if not ll_metrics_df.empty:\n",
    "        with open(kwargs['output_dir'] / (output + '.tsv'), 'w', newline='\\n') as metrics_file:\n",
    "            ll_metrics_df.set_index(['sg_name', 'agent_config', 'run_id']).sort_values(\n",
    "                ['agent_config', 'run_id']).to_csv(metrics_file, sep='\\t')\n",
    "    if ll_metrics_dicts:    \n",
    "        with open(kwargs['output_dir'] / (output + '.json'), 'w', newline='\\n') as metrics_file:\n",
    "            json.dump(ll_metrics_dicts, metrics_file)\n",
    "    if not log_data_df.empty:\n",
    "        log_data_df.reset_index(drop=True).to_feather(kwargs['output_dir'] / (output + '_data.feather'))\n",
    "\n",
    "# Save settings for evaluation\n",
    "if kwargs['do_save_settings']:\n",
    "    with open(kwargs['output_dir'] / (kwargs['output'] + '_settings.json'), 'w') as outfile:\n",
    "        kwargs['eval_dir'] = str(kwargs.get('eval_dir', ''))\n",
    "        kwargs['output_dir'] = str(kwargs.get('output_dir', ''))\n",
    "        json.dump(kwargs, outfile)"
   ]
  }
 ]
}