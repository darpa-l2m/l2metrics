{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-Lifetime Metrics Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2019 The Johns Hopkins University Applied Physics Laboratory LLC (JHU/APL).\n",
    "# All Rights Reserved. This material may be only be used, modified, or reproduced\n",
    "# by or for the U.S. Government pursuant to the license rights granted under the\n",
    "# clauses at DFARS 252.227-7013/7014 or FAR 52.227-14. For any other permission,\n",
    "# please contact the Office of Technology Transfer at JHU/APL.\n",
    "\n",
    "# NO WARRANTY, NO LIABILITY. THIS MATERIAL IS PROVIDED “AS IS.” JHU/APL MAKES NO\n",
    "# REPRESENTATION OR WARRANTY WITH RESPECT TO THE PERFORMANCE OF THE MATERIALS,\n",
    "# INCLUDING THEIR SAFETY, EFFECTIVENESS, OR COMMERCIAL VIABILITY, AND DISCLAIMS\n",
    "# ALL WARRANTIES IN THE MATERIAL, WHETHER EXPRESS OR IMPLIED, INCLUDING (BUT NOT\n",
    "# LIMITED TO) ANY AND ALL IMPLIED WARRANTIES OF PERFORMANCE, MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT OF INTELLECTUAL PROPERTY\n",
    "# OR OTHER THIRD PARTY RIGHTS. ANY USER OF THE MATERIAL ASSUMES THE ENTIRE RISK\n",
    "# AND LIABILITY FOR USING THE MATERIAL. IN NO EVENT SHALL JHU/APL BE LIABLE TO ANY\n",
    "# USER OF THE MATERIAL FOR ANY ACTUAL, INDIRECT, CONSEQUENTIAL, SPECIAL OR OTHER\n",
    "# DAMAGES ARISING FROM THE USE OF, OR INABILITY TO USE, THE MATERIAL, INCLUDING,\n",
    "# BUT NOT LIMITED TO, ANY DAMAGES FOR LOST PROFITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "from evaluation.evaluate import (compute_eval_metrics,\n",
    "                                 load_computational_costs,\n",
    "                                 load_performance_thresholds,\n",
    "                                 load_task_similarities, plot_summary,\n",
    "                                 unzip_logs)\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure metrics report\n",
    "sg_name = ''\n",
    "configuration = ''\n",
    "\n",
    "# Build file and directory strings\n",
    "eval_dir = Path('../../sg_' + sg_name + '_eval/m9_eval/')\n",
    "output_dir = Path('results/' + configuration +\n",
    "                  '/' + sg_name + '_' + configuration)\n",
    "output = sg_name + '_metrics_' + configuration\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "perf_measure = {\n",
    "    'argonne': 'score',\n",
    "    'hrl': 'norm_reward',\n",
    "    'sri': 'reward',\n",
    "    'teledyne': 'id_accuracy_cumulative',\n",
    "    'upenn': 'performance'\n",
    "}\n",
    "\n",
    "ste_dir = ''\n",
    "maintenance_method = 'both'\n",
    "transfer_method = 'both'\n",
    "normalization_method = 'task'\n",
    "do_unzip = False\n",
    "do_save_ste = False\n",
    "do_plot = True\n",
    "save_plots = True\n",
    "do_save = True\n",
    "\n",
    "# Generate other input arguments based on configuration\n",
    "do_smoothing = configuration in ['smoothed', 'normalized', 'normalized_no_outliers']\n",
    "do_normalize = configuration in ['normalized', 'normalized_no_outliers']\n",
    "remove_outliers = configuration in ['normalized_no_outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load computational cost data\n",
    "comp_cost_df = load_computational_costs(eval_dir)\n",
    "\n",
    "# Load performance threshold data\n",
    "perf_thresh_df = load_performance_thresholds(eval_dir)\n",
    "\n",
    "# Load task similarity data\n",
    "task_similarity_df = load_task_similarities(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip logs\n",
    "if do_unzip:\n",
    "    unzip_logs(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for lifelong learning logs\n",
    "ll_metrics_df, ll_metrics_dicts = compute_eval_metrics(eval_dir=eval_dir, ste_dir=ste_dir, output_dir=output_dir,\n",
    "                                                       perf_measure=perf_measure[sg_name],\n",
    "                                                       maintenance_method=maintenance_method,\n",
    "                                                       transfer_method=transfer_method,\n",
    "                                                       normalization_method=normalization_method,\n",
    "                                                       do_smoothing=do_smoothing, do_normalize=do_normalize,\n",
    "                                                       remove_outliers=remove_outliers, do_plot=do_plot,\n",
    "                                                       save_plots=save_plots, do_save_ste=do_save_ste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mean and standard deviation of data\n",
    "ll_metrics_df.drop(columns=['min', 'max']).groupby(\n",
    "    by=['complexity', 'difficulty']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show median and IQR of data\n",
    "ll_metrics_df.drop(columns=['min', 'max']).groupby(\n",
    "    by=['complexity', 'difficulty']).agg(['median', scipy.stats.iqr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aggregated data\n",
    "if do_plot:\n",
    "    %matplotlib ipympl\n",
    "    plot_summary(ll_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lifelong learning metrics DataFrame\n",
    "if do_save:\n",
    "    with open(output_dir.parent / (output + '.tsv'), 'w', newline='\\n') as metrics_file:\n",
    "        ll_metrics_df.set_index(['sg_name', 'agent_config', 'run_id']).sort_values(\n",
    "            ['agent_config', 'run_id']).to_csv(metrics_file, sep='\\t')\n",
    "    with open(output_dir.parent / (output + '.json'), 'w', newline='\\n') as metrics_file:\n",
    "        json.dump(ll_metrics_dicts, metrics_file)"
   ]
  }
 ]
}