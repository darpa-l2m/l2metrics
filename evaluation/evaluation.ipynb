{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-Lifetime Metrics Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import l2metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure metrics report\n",
    "perf_measure = 'performance'\n",
    "transfer_method = 'contrast'\n",
    "do_smoothing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify top-level directory for evaluation\n",
    "# The path should be that of an agent configuration directory within an evaluation directory\n",
    "# (e.g., m9_eval/agent_config-0/).\n",
    "log_dir = Path(\"example_eval/m9_eval/agent_config-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for STE logs\n",
    "ste_log_dir = log_dir / 'ste_logs' / 'ste_logs'\n",
    "\n",
    "if ste_log_dir.exists():\n",
    "    # Store all the STE data found in the directory\n",
    "    print('Storing STE data...')\n",
    "    for ste_dir in ste_log_dir.iterdir():\n",
    "        if ste_dir.is_dir():\n",
    "            l2metrics.util.save_ste_data(str(ste_dir))\n",
    "    print('Done storing STE data!\\n')\n",
    "else:\n",
    "    # STE log path not found - possibly because comrpressed archive has not been\n",
    "    # extracted in the same location yet\n",
    "    raise FileNotFoundError(f\"STE logs not found in expected location!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for LL logs\n",
    "ll_log_dir = log_dir / 'll_logs'\n",
    "\n",
    "if ll_log_dir.exists():\n",
    "    print('Computing metrics from LL logs...')\n",
    "\n",
    "    # Initialize LL metric dataframe\n",
    "    ll_metrics_df = pd.DataFrame()\n",
    "\n",
    "    # Compute and store the LL metrics for all scenarios found in the directory\n",
    "    for path in tqdm(list(ll_log_dir.iterdir()), desc='Overall'):\n",
    "        if path.is_dir():\n",
    "            for sub_path in tqdm(list(path.iterdir()), desc=path.name):\n",
    "                if sub_path.is_dir():\n",
    "                    scenario_dir = str(sub_path)\n",
    "\n",
    "                    # Initialize metrics report\n",
    "                    report = l2metrics.AgentMetricsReport(\n",
    "                        log_dir=scenario_dir, perf_measure=perf_measure,\n",
    "                        transfer_method=transfer_method, do_smoothing=do_smoothing)\n",
    "\n",
    "                    # Calculate metrics in order of their addition to the metrics list\n",
    "                    report.calculate()\n",
    "\n",
    "                    # Append lifetime metrics to dataframe\n",
    "                    ll_metrics_df = ll_metrics_df.append(\n",
    "                        report.lifetime_metrics_df, ignore_index=True)\n",
    "\n",
    "                    # Append scenario complexity and difficulty\n",
    "                    with open(sub_path / 'scenario_info.json', 'r') as json_file:\n",
    "                        scenario_info = json.load(json_file)\n",
    "                        if 'complexity' in scenario_info:\n",
    "                            ll_metrics_df.at[ll_metrics_df.index[-1], 'complexity'] = scenario_info['complexity']\n",
    "                        if 'difficulty' in scenario_info:\n",
    "                            ll_metrics_df.at[ll_metrics_df.index[-1], 'difficulty'] = scenario_info['difficulty']\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(f\"LL logs not found in expected location!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by complexity and difficulty\n",
    "ll_metrics_df = ll_metrics_df.sort_values(by=['complexity', 'difficulty'])\n",
    "ll_metrics_df.groupby(by=['complexity', 'difficulty']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_metrics_df.groupby(by=['complexity', 'difficulty']).agg(['median', scipy.stats.iqr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aggregated data\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "for index, metric in enumerate(ll_metrics_df.drop(columns=['complexity', 'difficulty']).columns, start=1):\n",
    "    # Create subplot for current metric\n",
    "    ax = fig.add_subplot(3, 3, index)\n",
    "\n",
    "    # Create grouped violin plot\n",
    "    sns.violinplot(x='complexity', y=metric, hue='difficulty', data=ll_metrics_df, palette='muted')\n",
    "\n",
    "    # Resize legend font\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize='8')\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize='6')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.35, hspace=0.35)"
   ]
  }
 ]
}